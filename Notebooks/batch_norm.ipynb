{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch norm\n",
    "1. Derivation\n",
    "2. Implementation\n",
    "3. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/fastai/Playground/PytorchStarter/Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a matrix $M$ of size $m \\times d$ goes through a columnwise operation with a vector of length $d$ in the forward pass, the error term for $M_{ij}$ is $m \\times $ the error term passed by a single element in the vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Z \\rightarrow$ calculate $\\mu, \\sigma \\rightarrow Z_{norm}$\n",
    "   $$ Z_{norm} = \\frac{Z - \\mu}{\\sigma} $$\n",
    "   where $\\mu, \\sigma$ are row vectors of length $d$\n",
    "2. $Z_{norm} \\rightarrow \\tilde{Z}$ \n",
    "   $$\\tilde{Z} = \\gamma \\cdot Z_{norm} + \\beta$$\n",
    "   where $\\gamma, \\beta$ are row vectors of length $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingLayer():\n",
    "    def __init__(self, tol=1e-6, eps=1e-3):\n",
    "        self.mu = None\n",
    "        self.std = None\n",
    "        self.centered = None\n",
    "        self.tol = tol\n",
    "    \n",
    "    def forward(self,z):\n",
    "        \"\"\"z: 2 dimentional numpy array of size (mxd)\"\"\"\n",
    "        if isinstance(z, torch.Tensor):\n",
    "            z = z.numpy()\n",
    "        m,d = z.shape\n",
    "        self.mu = np.mean(z,dim=0)\n",
    "        self.centered = z - self.mu\n",
    "        \n",
    "        self.var = np.var(z, dim=0) + eps\n",
    "#         var[abs(var)<tol] = eps\n",
    "        self.std = self.var**.5\n",
    "        self.inv = 1/self.std\n",
    "        \n",
    "        znorm = (z-self.mu) * self.inv\n",
    "        return znorm\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Computes dznorm/dz which has the same size as z\"\"\"\n",
    "        flow1 = self.centered*self.inv \n",
    "        flow2 = -self.centered*(-self.std**-2)*(0.5*self.var**(-0.5))*2*self.centered\n",
    "        return flow1 + flow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftScaleLayer():\n",
    "    def __init__(self, gamma, beta):\n",
    "        \"\"\"gamma: scale factor for each feature space. length d\n",
    "            beta: shift factor for each feature space. length d\n",
    "            Storage and computations are done in Numpy.\"\"\"\n",
    "        if isinstance(gamma, torch.Tensor): gamma = gamma.numpy()\n",
    "        if isinstance(beta, torch.Tensor): beta = beta.numpy()\n",
    "  \n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.out = None\n",
    "    def forward(self,z):\n",
    "        \"\"\"assumes z is a numpy array\"\"\"\n",
    "        self.out = self.gamma * z + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Returns [dout/dz, dout/dgamma, dout/dbeta]\"\"\"\n",
    "        dz = np.tile(self.gamma, (z.shape[0],1)) #dout/dz\n",
    "        dgamma = np.sum(z,dim=0)\n",
    "        dbeta = m * np.ones_like(beta)\n",
    "        return [dz, dgamma, dbeta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, inDim, H1, activation):\n",
    "        \"\"\" \n",
    "        Single layer binary classification network\n",
    "        Args: \n",
    "        ---\n",
    "        inDim: input dimension\n",
    "        H1: number of units in the first layer\n",
    "        activation: (torch.nn.modules.activation) activation function instance\n",
    "            eg.nn.Sigmoid() or nn.Relu()\n",
    "        \"\"\"\n",
    "        super(BCModel, self).__init__()\n",
    "        self.l1 = nn.Linear(inDim, H1)\n",
    "        self.l2 = nn.Linear(H1, 1)\n",
    "        \n",
    "        # Hidden layers' activation\n",
    "#         self.relu = nn.ReLU() #elementwise relu\n",
    "        self.activation = activation\n",
    "    \n",
    "        # todo: batch norm layer\n",
    "        self.bn1 = nn.BatchNorm1d(H1)\n",
    "        \n",
    "        # Last layer activation\n",
    "        self.sigmoid = nn.Sigmoid() #elementwise sigmoid activation\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"input and output are both tensors\"\"\"\n",
    "        \n",
    "        out1 = self.activation(self.bn1(self.l1(x))) #todo: add batch norm layer\n",
    "        out2 = self.activation(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "    def print_params(self):\n",
    "        for param in self.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   2.,   3.],\n",
      "        [  4.,   5.,   6.,   7.],\n",
      "        [  8.,   9.,  10.,  11.]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array(range(12), dtype=np.float32).reshape((3,-1))\n",
    "x = torch.from_numpy(x)\n",
    "print(x)\n",
    "N,D = x.shape\n",
    "bn = nn.BatchNorm1d(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  5.,  6.,  7.])\n",
      "tensor([ 4.0000,  4.0000,  4.0000,  4.0000])\n"
     ]
    }
   ],
   "source": [
    "mu = x.mean(0)\n",
    "std = (x.var(0) + bn.eps)**0.5\n",
    "print(mu)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -4., -4., -4.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 4.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "xnorm1 = (x-mu)/std; print(xnorm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if bn is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.2850,  0.7858,  0.2416,  0.6030]) Parameter containing:\n",
      "tensor([ 0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "gamma = bn.weight\n",
    "beta = bn.bias\n",
    "print(gamma,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1401, -3.1434, -0.9663, -2.4119],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.1401,  3.1434,  0.9663,  2.4119]])\n"
     ]
    }
   ],
   "source": [
    "xmanual = (x-mu)*gamma + beta; print(xmanual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3491, -0.9625, -0.2959, -0.7385],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3491,  0.9625,  0.2959,  0.7385]])\n"
     ]
    }
   ],
   "source": [
    "# compare manual computation with bn's forward function \n",
    "xauto = bn(x); print(xauto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,3); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1,:] = 2; a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  10.,  10.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10*torch.ones(1,3); m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  10.,  10.],\n",
       "        [ 20.,  20.,  20.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10.,  10.,  10.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.view(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  10.,  10.],\n",
       "        [ 20.,  20.,  20.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*m.view(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
