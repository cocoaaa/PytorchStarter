{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax output + Cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.testing as npt\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:100%\n",
       "        margin-left:5% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "\n",
       "\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't work with jupyterlab\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open('/Users/hjsong/Downloads/custom.css', 'r').read()\n",
    "    return HTML(styles)\n",
    "\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.eVec = None\n",
    "        self.qVec = None\n",
    "        self.s = None\n",
    "        \n",
    "    def forward(self, inVec):\n",
    "        self.eVec = np.exp(inVec)\n",
    "        self.s = self.eVec.sum()\n",
    "        self.qVec = self.eVec / self.s #todo: add a small epsilon?\n",
    "        self.check_qVec()\n",
    "        \n",
    "    def loss(self, y):\n",
    "        if self.qVec is None: \n",
    "            raise ValueError(\"Model must be trained before calling loss function\")#todo: unTrained model. \n",
    "        return np.sum( y*np.log(self.qVec)) \n",
    "\n",
    "    def backward(self, y):\n",
    "        if self.qVec is None:\n",
    "            raise ValueError(\"Model must be trained before backprop\")#todo: unTrained model. \n",
    "\n",
    "        dldqVec = y / self.qVec #todo: division by zero\n",
    "        \n",
    "        #D is a square mtx for gradient of qVector wrt eVector\n",
    "        D = self.build_dqde()\n",
    "        dldinVec = D.dot(dldqVec)\n",
    "        return dldinVec\n",
    "        \n",
    "    def build_dqde(self):\n",
    "        if self.eVec is None:\n",
    "            raise ValueError(\"Model must be trained before building D (=dq/de)\") #todo: unTrained model. \n",
    "            \n",
    "        dim = len(self.eVec)\n",
    "        D = np.ones(dim) * -self.eVec.reshape((dim,-1))\n",
    "        for i in range(dim):\n",
    "            D[i,i] = self.s - self.eVec[i]\n",
    "        return D / self.s**2\n",
    "    \n",
    "    def check_qVec(self):\n",
    "        \n",
    "        flag = math.isclose(np.sum(self.qVec),1)\n",
    "        if not flag:\n",
    "            raise ValueError(\"qVec does not add upto 1!\")\n",
    "        return flag\n",
    "\n",
    "    def reset(self):\n",
    "        self.eVec = None\n",
    "        self.qVec = None\n",
    "        self.s = None\n",
    "        \n",
    "    def print_state(self):\n",
    "        print(\"eVec: \\n\", self.eVec,\n",
    "              \"\\ns: \", self.s,\n",
    "             \"\\nqVec: \\n\", self.qVec)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test basic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:\n",
      " [[1]\n",
      " [5]\n",
      " [7]] \n",
      "evec:\n",
      " [[   2.71828183]\n",
      " [ 148.4131591 ]\n",
      " [1096.63315843]] \n",
      "s: 1247.7645993594942 \n",
      "qvec:\n",
      " [[0.00217852]\n",
      " [0.11894324]\n",
      " [0.87887824]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "x = np.array([1,5,7]).reshape((3,-1))\n",
    "softmax.forward(x)\n",
    "print(\"in:\\n\", x,\n",
    "      \"\\nevec:\\n\", softmax.eVec,\n",
    "      \"\\ns:\", softmax.s, \n",
    "      \"\\nqvec:\\n\", softmax.qVec)\n",
    "# print(np.sum(softmax.qVec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(softmax.check_qVec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eVec: \n",
      " [[   2.71828183]\n",
      " [ 148.4131591 ]\n",
      " [1096.63315843]] \n",
      "qVec: \n",
      " [[0.00217852]\n",
      " [0.11894324]\n",
      " [0.87887824]] \n",
      "s:  1247.7645993594942\n",
      "eVec: \n",
      " None \n",
      "qVec: \n",
      " None \n",
      "s:  None\n"
     ]
    }
   ],
   "source": [
    "softmax.print_state()\n",
    "softmax.reset()\n",
    "softmax.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test forward and backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eVec: \n",
      " [[2.20264658e+04]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]] \n",
      "qVec: \n",
      " [[9.99909208e-01]\n",
      " [4.53958078e-05]\n",
      " [4.53958078e-05]] \n",
      "s:  22028.465794806718\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.array([10,0,0]).reshape((3,1))\n",
    "softmax.forward(y_hat)\n",
    "softmax.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,0,0]).reshape((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.079573746728087e-05"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.loss(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.12155874e-09 -4.53916863e-05 -4.53916863e-05]\n",
      " [-2.06077937e-09  4.53937471e-05 -2.06077937e-09]\n",
      " [-2.06077937e-09 -2.06077937e-09  4.53937471e-05]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "D = softmax.build_dqde(); print(D); print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eVec: \n",
      " [[2.20264658e+04]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]] \n",
      "qVec: \n",
      " [[9.99909208e-01]\n",
      " [4.53958078e-05]\n",
      " [4.53958078e-05]] \n",
      "s:  22028.465794806718\n"
     ]
    }
   ],
   "source": [
    "softmax.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation (autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.tensor(x, dtype=torch.float); print(y_hat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd version \n",
    "class Softmax2(Softmax):\n",
    "    def __init__(self):\n",
    "        super(Softmax2, self).__init__()\n",
    "        \n",
    "    def forward(self, inVec):\n",
    "        #todo: use torch to calculate the forward pass. \n",
    "        # torch.tensor that requires a grad tracking cannot call .numpy\n",
    "        if not isinstance(inVec, torch.Tensor) or inVec.dtype != torch.float:\n",
    "            inVec = torch.tensor(inVec, dtype=torch.float)\n",
    "        if len(inVec.size()) == 1:\n",
    "            inVec = inVec.reshape(-1,t1.size()[0])\n",
    "        print('inVec:\\n', inVec)\n",
    "        self.eVec = torch.exp(inVec)\n",
    "        self.s = self.eVec.sum().item()\n",
    "        self.qVec = self.eVec / self.s\n",
    "        if self.check_qVec():\n",
    "            return self.qVec\n",
    "        \n",
    "    def check_qVec(self):\n",
    "        if self.qVec is None:\n",
    "            raise ValueError(\"Model is not trained yet.\")#todo: unTrained model. \n",
    "                \n",
    "        flag = math.isclose(self.qVec.sum().item(),1)\n",
    "        if not flag:\n",
    "            raise ValueError(\"qVec does not add upto 1!\")\n",
    "        return flag\n",
    "        \n",
    "        \n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    \"\"\"Computes the cross entropy loss between two probability \n",
    "    distribution, y_hat and y\"\"\"\n",
    "    if not isinstance(y_hat, torch.Tensor): y_hat = torch.tensor(y_hat, dtype=torch.float)\n",
    "    if not isinstance(y, torch.Tensor): y = torch.tensor(y, dtype=torch.float)\n",
    "        \n",
    "    return -torch.sum(y*torch.log(y_hat)) #must return a tensor object for autograd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "def test_softmax2():\n",
    "    model = Softmax2()\n",
    "    model.print_state()\n",
    "    \n",
    "    inVec = torch.tensor([1,2,3], dtype=torch.float)\n",
    "    qVec = model.forward(inVec)\n",
    "    model.print_state()\n",
    "    \n",
    "    print('output of forward:\\n', qVec)\n",
    "    \n",
    "def test_cross_entropy_loss():\n",
    "    y = torch.tensor([1,0,0], dtype=torch.float).reshape(3,1)\n",
    "    \n",
    "    y_perfect = torch.tensor([0.99999,0.000005,0.000005], dtype=torch.float).reshape(y.shape)\n",
    "    y_wrong = torch.tensor([0.00005,0.00005, 0.9999], dtype=torch.float).reshape(y.shape)\n",
    "    \n",
    "    print(\"Right: \", cross_entropy_loss(y_perfect, y).item() )\n",
    "    print(\"Wrong: \", cross_entropy_loss(y_wrong, y).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eVec: \n",
      " None \n",
      "s:  None \n",
      "qVec: \n",
      " None\n",
      "inVec:\n",
      " tensor([[ 1.,  2.,  3.]])\n",
      "eVec: \n",
      " tensor([[  2.7183,   7.3891,  20.0855]]) \n",
      "s:  30.192874908447266 \n",
      "qVec: \n",
      " tensor([[ 0.0900,  0.2447,  0.6652]])\n",
      "output of forward:\n",
      " tensor([[ 0.0900,  0.2447,  0.6652]])\n"
     ]
    }
   ],
   "source": [
    "test_softmax2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right:  1.0013630344474223e-05\n",
      "Wrong:  9.903487205505371\n"
     ]
    }
   ],
   "source": [
    "#Test `cross_entropy_loss`\n",
    "test_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper: Normalize an array-type object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "import math\n",
    "def normalize(x, eps=1e-6):\n",
    "    \"\"\"x: torch.tensor, numpy.array, or python list object.\n",
    "    Returns a normalized array as torch.tensor with dtype of torch.float\n",
    "    Note: Don't use it if x is a tensor that requires gradient backprop.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.numpy()\n",
    "    s = np.sum(x)\n",
    "    if math.isclose(s,0): s += eps\n",
    "    normed = x / s\n",
    "    assert(math.isclose(np.sum(normed),1))\n",
    "    return torch.tensor(normed, dtype=torch.float) # tensor (dtype=torch.float32) can't be divided by np.float32? strange!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the forward from the model and loss function as in lec 06 to use autograd for\n",
    "# autodiff. Then compare the two implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inVec:\n",
      " tensor([[ 1.,  2.,  3.]])\n",
      "Before backprop\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax2()\n",
    "inVec = torch.tensor([1,2,3], dtype=torch.float, requires_grad=True )\n",
    "qVec = softmax.forward(inVec)\n",
    "l = cross_entropy_loss(qVec, np.array([1,0,0]))\n",
    "print(\"Before backprop\")\n",
    "print(inVec.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4076)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backprop\n",
      "tensor([-1., -0., -0.])\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "print(\"After backprop\")\n",
    "print(inVec.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
